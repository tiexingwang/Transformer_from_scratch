from datasets import load_dataset
import torch
from torch.utils.data import Dataset, DataLoader
import json
import os

def check_datasets():
    """Download and check a small subset of translation data for learning"""
    print("ğŸ” Checking available datasets...")
    
    try:
        # Load a small subset of the WMT19 dataset for English-Chinese translation
        dataset = load_dataset("wmt19", "zh-en", split="train")
        print(f"âœ… Dataset loaded successfully!")
        print(f"ğŸ“Š Total samples: {len(dataset)}")
        
        # Take only a small subset for learning (first 1000 samples)
        small_dataset = dataset.select(range(min(1000, len(dataset))))
        print(f"ğŸ“š Using small subset: {len(small_dataset)} samples")
        
        # Show some examples
        print("\nğŸ“ Sample data:")
        for i in range(min(3, len(small_dataset))):
            print(f"Sample {i+1}:")
            print(f"  English: {small_dataset[i]['translation']['en']}")
            print(f"  Chinese: {small_dataset[i]['translation']['zh']}")
            print()
        
        # Save a small subset to local files for easy access
        save_small_dataset(small_dataset, max_samples=100)
        
        return small_dataset
        
    except Exception as e:
        print(f"âŒ Error loading dataset: {e}")
        print("ğŸ”„ Creating synthetic data for testing...")
        return create_synthetic_data()

def save_small_dataset(dataset, max_samples=100):
    """Save a small subset of the dataset to local files"""
    print(f"ğŸ’¾ Saving {max_samples} samples to local files...")
    
    # Create data directory
    os.makedirs("data", exist_ok=True)
    
    # Save training data
    with open("data/train.en-zh.txt", "w", encoding="utf-8") as f:
        for i in range(min(max_samples, len(dataset))):
            en_text = dataset[i]['translation']['en']
            zh_text = dataset[i]['translation']['zh']
            f.write(f"{en_text}\t{zh_text}\n")
    
    # Save validation data (next 20 samples)
    with open("data/val.en-zh.txt", "w", encoding="utf-8") as f:
        for i in range(max_samples, min(max_samples + 20, len(dataset))):
            en_text = dataset[i]['translation']['en']
            zh_text = dataset[i]['translation']['zh']
            f.write(f"{en_text}\t{zh_text}\n")
    
    print("âœ… Data saved to data/train.en-zh.txt and data/val.en-zh.txt")

def create_synthetic_data():
    """Create synthetic English-Chinese pairs for testing when dataset is unavailable"""
    print("ğŸ”§ Creating synthetic translation data...")
    
    synthetic_data = [
        ("Hello world", "ä½ å¥½ä¸–ç•Œ"),
        ("How are you", "ä½ å¥½å—"),
        ("I love you", "æˆ‘çˆ±ä½ "),
        ("Good morning", "æ—©ä¸Šå¥½"),
        ("Thank you", "è°¢è°¢"),
        ("Goodbye", "å†è§"),
        ("What is your name", "ä½ å«ä»€ä¹ˆåå­—"),
        ("I am learning", "æˆ‘åœ¨å­¦ä¹ "),
        ("Machine learning", "æœºå™¨å­¦ä¹ "),
        ("Artificial intelligence", "äººå·¥æ™ºèƒ½"),
        ("Deep learning", "æ·±åº¦å­¦ä¹ "),
        ("Neural network", "ç¥ç»ç½‘ç»œ"),
        ("Transformer model", "Transformeræ¨¡å‹"),
        ("Natural language processing", "è‡ªç„¶è¯­è¨€å¤„ç†"),
        ("Computer science", "è®¡ç®—æœºç§‘å­¦"),
        ("Programming", "ç¼–ç¨‹"),
        ("Python language", "Pythonè¯­è¨€"),
        ("Data science", "æ•°æ®ç§‘å­¦"),
        ("Big data", "å¤§æ•°æ®"),
        ("Cloud computing", "äº‘è®¡ç®—")
    ]
    
    # Save synthetic data
    os.makedirs("data", exist_ok=True)
    
    with open("data/train.en-zh.txt", "w", encoding="utf-8") as f:
        for en, zh in synthetic_data[:15]:  # First 15 for training
            f.write(f"{en}\t{zh}\n")
    
    with open("data/val.en-zh.txt", "w", encoding="utf-8") as f:
        for en, zh in synthetic_data[15:]:  # Last 5 for validation
            f.write(f"{en}\t{zh}\n")
    
    print("âœ… Synthetic data created!")
    print(f"ğŸ“š Training samples: 15")
    print(f"ğŸ“š Validation samples: 5")
    
    return synthetic_data

def test_data_loading():
    """Test if the saved data can be loaded correctly"""
    print("\nğŸ§ª Testing data loading...")
    
    try:
        # Load training data
        with open("data/train.en-zh.txt", "r", encoding="utf-8") as f:
            train_lines = f.readlines()
        
        # Load validation data
        with open("data/val.en-zh.txt", "r", encoding="utf-8") as f:
            val_lines = f.readlines()
        
        print(f"âœ… Training data: {len(train_lines)} samples")
        print(f"âœ… Validation data: {len(val_lines)} samples")
        
        # Show first few samples
        print("\nğŸ“ First training samples:")
        for i, line in enumerate(train_lines[:3]):
            en, zh = line.strip().split('\t')
            print(f"  {i+1}. English: {en}")
            print(f"     Chinese: {zh}")
            print()
        
        return True
        
    except Exception as e:
        print(f"âŒ Error loading saved data: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ Starting dataset preparation for Transformer learning...")
    print("=" * 60)
    
    # Download/check dataset
    dataset = check_datasets()
    
    # Test data loading
    test_data_loading()
    
    print("\nğŸ¯ Next steps:")
    print("1. Your data is ready in the 'data/' folder")
    print("2. You can now run the Transformer training")
    print("3. Start with a small model configuration for learning")
    print("\nğŸ’¡ Learning path:")
    print("   - First: Understand the data format")
    print("   - Second: Test the Transformer forward pass")
    print("   - Third: Train on the small dataset")
    print("   - Fourth: Experiment with different configurations")
